{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashalogic/Persian-Sentiment-Analyzer/blob/master/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMNPBU2OwSBw",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial : Persian Sentiment Analysis With LSTM\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Step by step from dataset to ready to use model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BELKe6-qixIA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download and load word embedding model\n",
        "modelName = \"Fasttext\" #@param [\"Fasttext\"]\n",
        "#@markdown ###Or you can use your own pretrained model\n",
        "modelURL = \"\" #@param {type:\"string\"}\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "!pip install fasttext\n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAQMVT05MMY4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download and prepare Dataset\n",
        "# !wget https://raw.githubusercontent.com/minasmz/Sentiment-Analysis-with-LSTM-in-Persian/master/totalReviewWithSuggestion.csv\n",
        "\n",
        "import pandas\n",
        "import random\n",
        "import numpy\n",
        "import hazm\n",
        "\n",
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "csv_dataset = pandas.read_csv(\"/content/totalReviewWithSuggestion.csv\")\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Text'],csv_dataset['Suggestion'])))\n",
        "pos=list(filter(lambda x: x[1] == 1,revlist))\n",
        "nat=list(filter(lambda x: x[1] == 2,revlist))\n",
        "neg=list(filter(lambda x: x[1] == 3,revlist))\n",
        "print(\"Posetive count {}\".format(len(pos)))\n",
        "print(\"Negetive count {}\".format(len(neg)))\n",
        "print(\"Natural  count {}\".format(len(nat)))\n",
        "print()\n",
        "print(\"Total    count {}\".format(len(revlist)))\n",
        "print()\n",
        "print(\"Posetive sample : \",\"\\n\",pos[random.randrange(1,len(pos))])\n",
        "print(\"Negetive sample : \",\"\\n\",neg[random.randrange(1,len(neg))])\n",
        "print(\"Natural  sample : \",\"\\n\",nat[random.randrange(1,len(nat))])\n",
        "\n",
        "revlist_shuffle = pos[:450] + neg[:450]\n",
        "random.shuffle(revlist_shuffle)\n",
        "print(len(revlist_shuffle))\n",
        "# Not Important\n",
        "# revdict = dict(zip(csv_dataset['Text'],csv_dataset['Suggestion']))\n",
        "# revlist = [ [k,v] for k, v in revdict.items() ]\n",
        "# labels = csv_dataset['Score']\n",
        "# labels2 = numpy.array([1 if each > 3 else 0 for each in labels])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUJceKehjfJ3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Prepare Train & Test Data\n",
        "vector_size = 300 #@param {type:\"integer\"}\n",
        "max_no_tokens = 20 #@param {type:\"integer\"}\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "train_size = int(0.9*(len(revlist_shuffle)))\n",
        "test_size = int(0.1*(len(revlist_shuffle)))\n",
        "\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False))\n",
        "\n",
        "x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvcGBpjPwFL0",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Fill X_Train, X_Test, Y_Train, Y_Test with Dataset\n",
        "for i, index in enumerate(indexes):\n",
        "  text_words = hazm.word_tokenize(revlist_shuffle[index][0])\n",
        "  for t in range(0,len(text_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    \n",
        "    if text_words[t] not in model.words:\n",
        "      continue\n",
        "    if i < train_size:\n",
        "      x_train[i, t, :] = model.get_word_vector(text_words[t])\n",
        "    else:\n",
        "      x_test[i - train_size, t, :] = model.get_word_vector(text_words[t])\n",
        "\n",
        "  if i < train_size:\n",
        "    y_train[i, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[i - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "    \n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mbfwpab3Yb8",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Set batchSize and epochs\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "no_epochs = 100 #@param {type:\"integer\"}\n",
        "w2v_model = model\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1z_mq913jTq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Prepare LSTM Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n",
        "                 input_shape=(max_no_tokens, vector_size)))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "\n",
        "model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
        "\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9qfU9CM3mL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,\n",
        "         validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TKsHaoO7HpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpocYJkB7KMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xPkhZX7Qmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('twitter-sentiment-fasttext.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kLhLv1h7UD_",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "3a043016-4a37-4470-b71b-ca4cca08feff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#@title Lets test our model with complex negative Review !\n",
        "user_text = \"\\u0628\\u0627\\u062A\\u0631\\u06CC\\u0634 \\u0632\\u0648\\u062F \\u062E\\u0627\\u0644\\u06CC \\u0645\\u06CC\\u0634\\u0647 \\u062F\\u0648\\u0631\\u0628\\u06CC\\u0646 \\u0627\\u0634 \\u06A9\\u06CC\\u0641\\u06CC\\u062A \\u0627\\u0634 \\u062F\\u0631 \\u062D\\u062F 13 \\u0646\\u06CC\\u0633\\u062A \\u0647\\u0646\\u062F\\u0641\\u0631\\u06CC \\u06A9\\u0647 \\u062F\\u0627\\u062E\\u0644\\u0634 \\u06AF\\u0630\\u0627\\u0634\\u062A\\u0646 \\u0648\\u0627\\u0642\\u0639\\u0627 \\u0628\\u06CC \\u06A9\\u06CC\\u0641\\u06CC\\u062A\\u0647 \\u060C \\u0645\\u0627\\u0644 \\u0646\\u0648\\u06A9\\u06CC\\u064700 \\u0647\\u0645\\u06CC\\u0646\\u0637\\u0648\\u0631 \\u0634\\u0627\\u0631\\u0698\\u0631 \\u0648\\u0644\\u06CC \\u062E\\u0628 \\u062F\\u0627\\u062E\\u0644\\u0634 \\u06CC\\u0647 \\u0642\\u0627\\u0628 \\u0698\\u0644\\u0647 \\u0627\\u06CC \\u0648 \\u06AF\\u0644\\u0633 \\u0645\\u0639\\u0645\\u0648\\u0644\\u06CC \\u0628\\u0648\\u062F \\u0634\\u0627\\u06CC\\u062F \\u0627\\u06AF\\u0631 \\u0648\\u0636\\u0639\\u06CC\\u062A \\u0627\\u0642\\u062A\\u0635\\u0627\\u062F \\u0627\\u06CC\\u0646\\u0637\\u0648\\u0631\\u06CC \\u0646\\u0628\\u0648\\u062F \\u0646\\u0635\\u0641 \\u0642\\u06CC\\u0645\\u062A \\u0645\\u06CC\\u0634\\u062F \\u062E\\u0631\\u06CC\\u062F\\u0634\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = _wordtokenizer.tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>1 % </h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>98 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "outputId": "7c3d7b21-607a-4fc6-da5b-124f63ec9c1e",
        "id": "xXt5rQ0qmyax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#@title Lets test our model with complex negative Review !\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = _wordtokenizer.tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>93 % </h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>6 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}