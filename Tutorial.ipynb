{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashalogic/Persian-Sentiment-Analyzer/blob/master/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMNPBU2OwSBw",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial : Persian Sentiment Analysis With LSTM\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Step by step from dataset to ready to use model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BELKe6-qixIA",
        "colab_type": "code",
        "outputId": "531f4319-d8a7-424b-e382-8c73abb6f1e9",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "#@title Download and load word embedding model\n",
        "modelName = \"Fasttext\" #@param [\"Fasttext\"]\n",
        "#@markdown ###Or you can use your own pretrained model\n",
        "modelURL = \"\" #@param {type:\"string\"}\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gunzip /content/cc.fa.300.bin.gz\n",
        "!pip install fasttext\n",
        "\n",
        "import fasttext \n",
        "\n",
        "%time\n",
        "model = fasttext.load_model(\"/content/cc.fa.300.bin\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-01 16:33:33--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.22.166, 104.20.6.166, 2606:4700:10::6814:16a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.22.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4502524724 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.fa.300.bin.gz’\n",
            "\n",
            "cc.fa.300.bin.gz    100%[===================>]   4.19G  11.2MB/s    in 6m 15s  \n",
            "\n",
            "2019-10-01 16:39:48 (11.5 MB/s) - ‘cc.fa.300.bin.gz’ saved [4502524724/4502524724]\n",
            "\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.4.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (41.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.16.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2385569 sha256=b9e13511bfec9c50f9b82bea8636f62341db64bb18099c0371d125e25ec83f69\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n",
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 4.77 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAQMVT05MMY4",
        "colab_type": "code",
        "outputId": "77dcc5aa-f938-4fec-b9ac-a14653227d64",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/minasmz/Sentiment-Analysis-with-LSTM-in-Persian/master/totalReviewWithSuggestion.csv\n",
        "\n",
        "import pandas\n",
        "import random\n",
        "import numpy\n",
        "import hazm\n",
        "\n",
        "def CleanPersianText(text):\n",
        "  _normalizer = hazm.Normalizer()\n",
        "  text = _normalizer.normalize(text)\n",
        "  return text\n",
        "\n",
        "csv_dataset = pandas.read_csv(\"/content/totalReviewWithSuggestion.csv\")\n",
        "revlist = list(map(lambda x: [CleanPersianText(x[0]),x[1]],zip(csv_dataset['Text'],csv_dataset['Suggestion'])))\n",
        "pos=list(filter(lambda x: x[1] == 1,revlist))\n",
        "nat=list(filter(lambda x: x[1] == 2,revlist))\n",
        "neg=list(filter(lambda x: x[1] == 3,revlist))\n",
        "print(\"Posetive count {}\".format(len(pos)))\n",
        "print(\"Negetive count {}\".format(len(neg)))\n",
        "print(\"Natural  count {}\".format(len(nat)))\n",
        "print()\n",
        "print(\"Total    count {}\".format(len(revlist)))\n",
        "print()\n",
        "print(\"Posetive sample : \",\"\\n\",pos[random.randrange(1,len(pos))])\n",
        "print(\"Negetive sample : \",\"\\n\",neg[random.randrange(1,len(neg))])\n",
        "print(\"Natural  sample : \",\"\\n\",nat[random.randrange(1,len(nat))])\n",
        "\n",
        "revlist_shuffle = pos[:450] + neg[:450]\n",
        "random.shuffle(revlist_shuffle)\n",
        "print(len(revlist_shuffle))\n",
        "# Not Important\n",
        "# revdict = dict(zip(csv_dataset['Text'],csv_dataset['Suggestion']))\n",
        "# revlist = [ [k,v] for k, v in revdict.items() ]\n",
        "# labels = csv_dataset['Score']\n",
        "# labels2 = numpy.array([1 if each > 3 else 0 for each in labels])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Posetive count 2382\n",
            "Negetive count 460\n",
            "Natural  count 419\n",
            "\n",
            "Total    count 3261\n",
            "\n",
            "Posetive sample :  \n",
            " ['عالیه اینو بخرید سراغ هیچ تابه دیگه\\u200cای نمیرید شاید فکر کنید قیمتش بالاست اما واقعا ارزش پولی که میدینو بهتون برمیگردونه ', 1]\n",
            "Negetive sample :  \n",
            " ['حتما بخرید عالیه ', 3]\n",
            "Natural  sample :  \n",
            " ['تصویر زنده اصلا نمیدونی فیلمه یا واقعیت ', 2]\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUJceKehjfJ3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Prepare Train & Test Data\n",
        "vector_size = 300 #@param {type:\"integer\"}\n",
        "max_no_tokens = 20 #@param {type:\"integer\"}\n",
        "import numpy as np\n",
        "import keras.backend as K\n",
        "train_size = int(0.9*(len(revlist_shuffle)))\n",
        "test_size = int(0.1*(len(revlist_shuffle)))\n",
        "\n",
        "indexes = set(np.random.choice(len(revlist_shuffle), train_size + test_size, replace=False))\n",
        "\n",
        "x_train = np.zeros((train_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_train = np.zeros((train_size, 2), dtype=np.int32)\n",
        "\n",
        "x_test = np.zeros((test_size, max_no_tokens, vector_size), dtype=K.floatx())\n",
        "y_test = np.zeros((test_size, 2), dtype=np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvcGBpjPwFL0",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "93826aa8-3407-4eb6-df79-f547962165cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#@title Fill X_Train, X_Test, Y_Train, Y_Test with Dataset\n",
        "for i, index in enumerate(indexes):\n",
        "  text_words = hazm.word_tokenize(revlist_shuffle[index][0])\n",
        "  for t in range(0,len(text_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    \n",
        "    if text_words[t] not in model.words:\n",
        "      continue\n",
        "    if i < train_size:\n",
        "      x_train[i, t, :] = model.get_word_vector(text_words[t])\n",
        "    else:\n",
        "      x_test[i - train_size, t, :] = model.get_word_vector(text_words[t])\n",
        "\n",
        "  if i < train_size:\n",
        "    y_train[i, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "  else:\n",
        "    y_test[i - train_size, :] = [1.0, 0.0] if revlist_shuffle[index][1] == 3 else [0.0, 1.0]\n",
        "    \n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((810, 20, 300), (90, 20, 300), (810, 2), (90, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Mbfwpab3Yb8",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Set batchSize and epochs\n",
        "batch_size = 500 #@param {type:\"integer\"}\n",
        "no_epochs = 100 #@param {type:\"integer\"}\n",
        "w2v_model = model\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1z_mq913jTq",
        "colab_type": "code",
        "outputId": "894041ef-a7d1-4240-b343-e33cfbfa89f4",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        }
      },
      "source": [
        "#@title Prepare LSTM Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, Dropout, Dense, Flatten, LSTM, MaxPooling1D, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, TensorBoard\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same',\n",
        "                 input_shape=(max_no_tokens, vector_size)))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='elu', padding='same'))\n",
        "model.add(Conv1D(32, kernel_size=3, activation='relu', padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=3))\n",
        "\n",
        "model.add(Bidirectional(LSTM(512, dropout=0.2, recurrent_dropout=0.3)))\n",
        "\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(512, activation='sigmoid'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='logs/', histogram_freq=0, write_graph=True, write_images=True)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 20, 32)            28832     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 20, 32)            3104      \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 20, 32)            3104      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 6, 32)             0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 1024)              2232320   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 3,318,498\n",
            "Trainable params: 3,318,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9qfU9CM3mL-",
        "colab_type": "code",
        "outputId": "9cee8570-8083-4e04-cf6e-5185f2f45571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, shuffle=True, epochs=no_epochs,\n",
        "         validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 810 samples, validate on 90 samples\n",
            "Epoch 1/100\n",
            "810/810 [==============================] - 0s 283us/step - loss: 0.7479 - acc: 0.4963 - val_loss: 0.6936 - val_acc: 0.4889\n",
            "Epoch 2/100\n",
            "810/810 [==============================] - 0s 244us/step - loss: 0.7513 - acc: 0.4877 - val_loss: 0.6933 - val_acc: 0.5111\n",
            "Epoch 3/100\n",
            "810/810 [==============================] - 0s 238us/step - loss: 0.7344 - acc: 0.5123 - val_loss: 0.6935 - val_acc: 0.5111\n",
            "Epoch 4/100\n",
            "810/810 [==============================] - 0s 230us/step - loss: 0.7505 - acc: 0.4753 - val_loss: 0.6928 - val_acc: 0.5111\n",
            "Epoch 5/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.7459 - acc: 0.5037 - val_loss: 0.6943 - val_acc: 0.4889\n",
            "Epoch 6/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.7360 - acc: 0.5037 - val_loss: 0.6953 - val_acc: 0.4889\n",
            "Epoch 7/100\n",
            "810/810 [==============================] - 0s 228us/step - loss: 0.7463 - acc: 0.4827 - val_loss: 0.6944 - val_acc: 0.4889\n",
            "Epoch 8/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.7269 - acc: 0.5111 - val_loss: 0.6930 - val_acc: 0.5111\n",
            "Epoch 9/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7251 - acc: 0.5037 - val_loss: 0.6927 - val_acc: 0.5111\n",
            "Epoch 10/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.7324 - acc: 0.5173 - val_loss: 0.6928 - val_acc: 0.5111\n",
            "Epoch 11/100\n",
            "810/810 [==============================] - 0s 224us/step - loss: 0.7337 - acc: 0.4877 - val_loss: 0.6932 - val_acc: 0.4889\n",
            "Epoch 12/100\n",
            "810/810 [==============================] - 0s 219us/step - loss: 0.7182 - acc: 0.5309 - val_loss: 0.6939 - val_acc: 0.4889\n",
            "Epoch 13/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.7241 - acc: 0.5111 - val_loss: 0.6941 - val_acc: 0.4889\n",
            "Epoch 14/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7407 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.4889\n",
            "Epoch 15/100\n",
            "810/810 [==============================] - 0s 211us/step - loss: 0.7325 - acc: 0.4988 - val_loss: 0.6926 - val_acc: 0.5111\n",
            "Epoch 16/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.7248 - acc: 0.4975 - val_loss: 0.6924 - val_acc: 0.5111\n",
            "Epoch 17/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.7478 - acc: 0.4716 - val_loss: 0.6923 - val_acc: 0.5111\n",
            "Epoch 18/100\n",
            "810/810 [==============================] - 0s 230us/step - loss: 0.7165 - acc: 0.5407 - val_loss: 0.6927 - val_acc: 0.4889\n",
            "Epoch 19/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.7432 - acc: 0.4914 - val_loss: 0.6930 - val_acc: 0.4889\n",
            "Epoch 20/100\n",
            "810/810 [==============================] - 0s 226us/step - loss: 0.7250 - acc: 0.5086 - val_loss: 0.6927 - val_acc: 0.4889\n",
            "Epoch 21/100\n",
            "810/810 [==============================] - 0s 215us/step - loss: 0.7037 - acc: 0.5531 - val_loss: 0.6922 - val_acc: 0.4889\n",
            "Epoch 22/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7437 - acc: 0.4691 - val_loss: 0.6916 - val_acc: 0.7000\n",
            "Epoch 23/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.7297 - acc: 0.5037 - val_loss: 0.6910 - val_acc: 0.5111\n",
            "Epoch 24/100\n",
            "810/810 [==============================] - 0s 212us/step - loss: 0.7310 - acc: 0.5000 - val_loss: 0.6906 - val_acc: 0.5111\n",
            "Epoch 25/100\n",
            "810/810 [==============================] - 0s 226us/step - loss: 0.7236 - acc: 0.5284 - val_loss: 0.6908 - val_acc: 0.5000\n",
            "Epoch 26/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.7210 - acc: 0.5284 - val_loss: 0.6905 - val_acc: 0.4889\n",
            "Epoch 27/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7182 - acc: 0.5111 - val_loss: 0.6893 - val_acc: 0.5556\n",
            "Epoch 28/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.7338 - acc: 0.4889 - val_loss: 0.6879 - val_acc: 0.6556\n",
            "Epoch 29/100\n",
            "810/810 [==============================] - 0s 228us/step - loss: 0.7233 - acc: 0.5062 - val_loss: 0.6865 - val_acc: 0.7333\n",
            "Epoch 30/100\n",
            "810/810 [==============================] - 0s 228us/step - loss: 0.7344 - acc: 0.4988 - val_loss: 0.6846 - val_acc: 0.7333\n",
            "Epoch 31/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.7171 - acc: 0.5173 - val_loss: 0.6825 - val_acc: 0.7000\n",
            "Epoch 32/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7055 - acc: 0.5284 - val_loss: 0.6803 - val_acc: 0.6111\n",
            "Epoch 33/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.7195 - acc: 0.4938 - val_loss: 0.6770 - val_acc: 0.6111\n",
            "Epoch 34/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.7088 - acc: 0.5272 - val_loss: 0.6709 - val_acc: 0.7000\n",
            "Epoch 35/100\n",
            "810/810 [==============================] - 0s 231us/step - loss: 0.6896 - acc: 0.5543 - val_loss: 0.6638 - val_acc: 0.7222\n",
            "Epoch 36/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.6936 - acc: 0.5580 - val_loss: 0.6555 - val_acc: 0.7222\n",
            "Epoch 37/100\n",
            "810/810 [==============================] - 0s 226us/step - loss: 0.6964 - acc: 0.5444 - val_loss: 0.6459 - val_acc: 0.7333\n",
            "Epoch 38/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.6679 - acc: 0.5728 - val_loss: 0.6357 - val_acc: 0.7222\n",
            "Epoch 39/100\n",
            "810/810 [==============================] - 0s 215us/step - loss: 0.6447 - acc: 0.6309 - val_loss: 0.6222 - val_acc: 0.7222\n",
            "Epoch 40/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.6366 - acc: 0.6296 - val_loss: 0.6049 - val_acc: 0.7444\n",
            "Epoch 41/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.6194 - acc: 0.6704 - val_loss: 0.5882 - val_acc: 0.7333\n",
            "Epoch 42/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.5857 - acc: 0.7111 - val_loss: 0.5757 - val_acc: 0.7444\n",
            "Epoch 43/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.5670 - acc: 0.7111 - val_loss: 0.5735 - val_acc: 0.7444\n",
            "Epoch 44/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.5478 - acc: 0.7358 - val_loss: 0.5660 - val_acc: 0.7333\n",
            "Epoch 45/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.5428 - acc: 0.7457 - val_loss: 0.5542 - val_acc: 0.7556\n",
            "Epoch 46/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.5346 - acc: 0.7444 - val_loss: 0.5491 - val_acc: 0.7556\n",
            "Epoch 47/100\n",
            "810/810 [==============================] - 0s 225us/step - loss: 0.5023 - acc: 0.7753 - val_loss: 0.5574 - val_acc: 0.7333\n",
            "Epoch 48/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.4952 - acc: 0.7765 - val_loss: 0.5563 - val_acc: 0.7333\n",
            "Epoch 49/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.4892 - acc: 0.7926 - val_loss: 0.5464 - val_acc: 0.7556\n",
            "Epoch 50/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.4881 - acc: 0.7938 - val_loss: 0.5447 - val_acc: 0.7778\n",
            "Epoch 51/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.4882 - acc: 0.7914 - val_loss: 0.5500 - val_acc: 0.7556\n",
            "Epoch 52/100\n",
            "810/810 [==============================] - 0s 219us/step - loss: 0.4752 - acc: 0.8012 - val_loss: 0.5502 - val_acc: 0.7667\n",
            "Epoch 53/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.4709 - acc: 0.7926 - val_loss: 0.5458 - val_acc: 0.7667\n",
            "Epoch 54/100\n",
            "810/810 [==============================] - 0s 215us/step - loss: 0.4472 - acc: 0.8025 - val_loss: 0.5418 - val_acc: 0.7667\n",
            "Epoch 55/100\n",
            "810/810 [==============================] - 0s 215us/step - loss: 0.4669 - acc: 0.8000 - val_loss: 0.5409 - val_acc: 0.7667\n",
            "Epoch 56/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.4610 - acc: 0.8111 - val_loss: 0.5356 - val_acc: 0.7556\n",
            "Epoch 57/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.4394 - acc: 0.8111 - val_loss: 0.5253 - val_acc: 0.7556\n",
            "Epoch 58/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.4390 - acc: 0.8235 - val_loss: 0.5216 - val_acc: 0.7444\n",
            "Epoch 59/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.4035 - acc: 0.8296 - val_loss: 0.5219 - val_acc: 0.7667\n",
            "Epoch 60/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.4388 - acc: 0.8185 - val_loss: 0.5198 - val_acc: 0.7667\n",
            "Epoch 61/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.4336 - acc: 0.8198 - val_loss: 0.5138 - val_acc: 0.7556\n",
            "Epoch 62/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.4290 - acc: 0.8247 - val_loss: 0.5134 - val_acc: 0.7667\n",
            "Epoch 63/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.4103 - acc: 0.8395 - val_loss: 0.5197 - val_acc: 0.7778\n",
            "Epoch 64/100\n",
            "810/810 [==============================] - 0s 232us/step - loss: 0.3870 - acc: 0.8383 - val_loss: 0.5229 - val_acc: 0.7889\n",
            "Epoch 65/100\n",
            "810/810 [==============================] - 0s 230us/step - loss: 0.4131 - acc: 0.8309 - val_loss: 0.5194 - val_acc: 0.7667\n",
            "Epoch 66/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.4066 - acc: 0.8420 - val_loss: 0.5178 - val_acc: 0.7778\n",
            "Epoch 67/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.3849 - acc: 0.8543 - val_loss: 0.5221 - val_acc: 0.7889\n",
            "Epoch 68/100\n",
            "810/810 [==============================] - 0s 214us/step - loss: 0.3862 - acc: 0.8617 - val_loss: 0.5317 - val_acc: 0.7778\n",
            "Epoch 69/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.3858 - acc: 0.8593 - val_loss: 0.5308 - val_acc: 0.7778\n",
            "Epoch 70/100\n",
            "810/810 [==============================] - 0s 219us/step - loss: 0.3865 - acc: 0.8543 - val_loss: 0.5239 - val_acc: 0.7889\n",
            "Epoch 71/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.4020 - acc: 0.8457 - val_loss: 0.5266 - val_acc: 0.7889\n",
            "Epoch 72/100\n",
            "810/810 [==============================] - 0s 220us/step - loss: 0.3687 - acc: 0.8679 - val_loss: 0.5336 - val_acc: 0.7889\n",
            "Epoch 73/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.3761 - acc: 0.8543 - val_loss: 0.5334 - val_acc: 0.7889\n",
            "Epoch 74/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.3655 - acc: 0.8667 - val_loss: 0.5255 - val_acc: 0.8111\n",
            "Epoch 75/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.3566 - acc: 0.8654 - val_loss: 0.5254 - val_acc: 0.7778\n",
            "Epoch 76/100\n",
            "810/810 [==============================] - 0s 222us/step - loss: 0.3636 - acc: 0.8741 - val_loss: 0.5294 - val_acc: 0.8000\n",
            "Epoch 77/100\n",
            "810/810 [==============================] - 0s 223us/step - loss: 0.3415 - acc: 0.8691 - val_loss: 0.5362 - val_acc: 0.8111\n",
            "Epoch 78/100\n",
            "810/810 [==============================] - 0s 224us/step - loss: 0.3511 - acc: 0.8728 - val_loss: 0.5425 - val_acc: 0.8000\n",
            "Epoch 79/100\n",
            "810/810 [==============================] - 0s 223us/step - loss: 0.3405 - acc: 0.8679 - val_loss: 0.5419 - val_acc: 0.8000\n",
            "Epoch 80/100\n",
            "810/810 [==============================] - 0s 213us/step - loss: 0.3479 - acc: 0.8654 - val_loss: 0.5438 - val_acc: 0.8000\n",
            "Epoch 81/100\n",
            "810/810 [==============================] - 0s 229us/step - loss: 0.3424 - acc: 0.8741 - val_loss: 0.5483 - val_acc: 0.7889\n",
            "Epoch 82/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.3329 - acc: 0.8877 - val_loss: 0.5532 - val_acc: 0.7778\n",
            "Epoch 83/100\n",
            "810/810 [==============================] - 0s 219us/step - loss: 0.3352 - acc: 0.8889 - val_loss: 0.5600 - val_acc: 0.7667\n",
            "Epoch 84/100\n",
            "810/810 [==============================] - 0s 211us/step - loss: 0.3350 - acc: 0.8778 - val_loss: 0.5688 - val_acc: 0.7556\n",
            "Epoch 85/100\n",
            "810/810 [==============================] - 0s 212us/step - loss: 0.3183 - acc: 0.8840 - val_loss: 0.5809 - val_acc: 0.7444\n",
            "Epoch 86/100\n",
            "810/810 [==============================] - 0s 213us/step - loss: 0.3145 - acc: 0.8901 - val_loss: 0.5792 - val_acc: 0.7444\n",
            "Epoch 87/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.3150 - acc: 0.8963 - val_loss: 0.5824 - val_acc: 0.7444\n",
            "Epoch 88/100\n",
            "810/810 [==============================] - 0s 216us/step - loss: 0.3158 - acc: 0.8877 - val_loss: 0.5988 - val_acc: 0.7444\n",
            "Epoch 89/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.3173 - acc: 0.8889 - val_loss: 0.6167 - val_acc: 0.7444\n",
            "Epoch 90/100\n",
            "810/810 [==============================] - 0s 221us/step - loss: 0.3004 - acc: 0.8938 - val_loss: 0.6021 - val_acc: 0.7444\n",
            "Epoch 91/100\n",
            "810/810 [==============================] - 0s 227us/step - loss: 0.3099 - acc: 0.8877 - val_loss: 0.6093 - val_acc: 0.7444\n",
            "Epoch 92/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.2922 - acc: 0.8975 - val_loss: 0.6108 - val_acc: 0.7556\n",
            "Epoch 93/100\n",
            "810/810 [==============================] - 0s 225us/step - loss: 0.3028 - acc: 0.8914 - val_loss: 0.6230 - val_acc: 0.7444\n",
            "Epoch 94/100\n",
            "810/810 [==============================] - 0s 210us/step - loss: 0.2808 - acc: 0.9025 - val_loss: 0.6295 - val_acc: 0.7333\n",
            "Epoch 95/100\n",
            "810/810 [==============================] - 0s 219us/step - loss: 0.2989 - acc: 0.9086 - val_loss: 0.6297 - val_acc: 0.7444\n",
            "Epoch 96/100\n",
            "810/810 [==============================] - 0s 226us/step - loss: 0.2826 - acc: 0.9049 - val_loss: 0.6403 - val_acc: 0.7333\n",
            "Epoch 97/100\n",
            "810/810 [==============================] - 0s 217us/step - loss: 0.2787 - acc: 0.9037 - val_loss: 0.6498 - val_acc: 0.7333\n",
            "Epoch 98/100\n",
            "810/810 [==============================] - 0s 218us/step - loss: 0.2710 - acc: 0.9037 - val_loss: 0.6396 - val_acc: 0.7556\n",
            "Epoch 99/100\n",
            "810/810 [==============================] - 0s 215us/step - loss: 0.2847 - acc: 0.9148 - val_loss: 0.6480 - val_acc: 0.7556\n",
            "Epoch 100/100\n",
            "810/810 [==============================] - 0s 224us/step - loss: 0.2703 - acc: 0.9111 - val_loss: 0.6716 - val_acc: 0.7111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51393ff9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TKsHaoO7HpW",
        "colab_type": "code",
        "outputId": "23f72e19-1140-4949-f1e0-d3202ea829ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpocYJkB7KMs",
        "colab_type": "code",
        "outputId": "13ce4294-ca2d-425f-a2bd-84d557eadc3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model.evaluate(x=x_test, y=y_test, batch_size=32, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "90/90 [==============================] - 0s 764us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6716464002927144, 0.7111111058129205]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xPkhZX7Qmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('twitter-sentiment-fasttext.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kLhLv1h7UD_",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "c567513b-1ce1-48bc-e72b-d6830c3a430f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#@title Lets test our model with complex negative Review !\n",
        "user_text = \"\\u0628\\u0627\\u062A\\u0631\\u06CC\\u0634 \\u0632\\u0648\\u062F \\u062E\\u0627\\u0644\\u06CC \\u0645\\u06CC\\u0634\\u0647 \\u062F\\u0648\\u0631\\u0628\\u06CC\\u0646 \\u0627\\u0634 \\u06A9\\u06CC\\u0641\\u06CC\\u062A \\u0627\\u0634 \\u062F\\u0631 \\u062D\\u062F 13 \\u0646\\u06CC\\u0633\\u062A \\u0647\\u0646\\u062F\\u0641\\u0631\\u06CC \\u06A9\\u0647 \\u062F\\u0627\\u062E\\u0644\\u0634 \\u06AF\\u0630\\u0627\\u0634\\u062A\\u0646 \\u0648\\u0627\\u0642\\u0639\\u0627 \\u0628\\u06CC \\u06A9\\u06CC\\u0641\\u06CC\\u062A\\u0647 \\u060C \\u0645\\u0627\\u0644 \\u0646\\u0648\\u06A9\\u06CC\\u064700 \\u0647\\u0645\\u06CC\\u0646\\u0637\\u0648\\u0631 \\u0634\\u0627\\u0631\\u0698\\u0631 \\u0648\\u0644\\u06CC \\u062E\\u0628 \\u062F\\u0627\\u062E\\u0644\\u0634 \\u06CC\\u0647 \\u0642\\u0627\\u0628 \\u0698\\u0644\\u0647 \\u0627\\u06CC \\u0648 \\u06AF\\u0644\\u0633 \\u0645\\u0639\\u0645\\u0648\\u0644\\u06CC \\u0628\\u0648\\u062F \\u0634\\u0627\\u06CC\\u062F \\u0627\\u06AF\\u0631 \\u0648\\u0636\\u0639\\u06CC\\u062A \\u0627\\u0642\\u062A\\u0635\\u0627\\u062F \\u0627\\u06CC\\u0646\\u0637\\u0648\\u0631\\u06CC \\u0646\\u0628\\u0648\\u062F \\u0646\\u0635\\u0641 \\u0642\\u06CC\\u0645\\u062A \\u0645\\u06CC\\u0634\\u062F \\u062E\\u0631\\u06CC\\u062F\\u0634\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = _wordtokenizer.tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>1 % </h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>98 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "outputId": "7c3d7b21-607a-4fc6-da5b-124f63ec9c1e",
        "id": "xXt5rQ0qmyax",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#@title Lets test our model with complex negative Review !\n",
        "user_text = \"\\u062E\\u06CC\\u0644\\u06CC \\u06AF\\u0648\\u0634\\u06CC\\u0647 \\u062E\\u0648\\u0628\\u06CC\\u0647. \\u062A\\u0634\\u062E\\u06CC\\u0635 \\u0686\\u0647\\u0631\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u062F\\u0627\\u062E\\u0644 \\u062C\\u0639\\u0628\\u0647 \\u06A9\\u0627\\u0648\\u0631 \\u06AF\\u0648\\u0634\\u06CC \\u0648 \\u0645\\u062D\\u0627\\u0641\\u0638 \\u0635\\u0641\\u062D\\u0647 \\u062F\\u0627\\u0631\\u0647. \\u0645\\u0646 \\u062F\\u06CC\\u0631\\u0648\\u0632 \\u0628\\u0647 \\u062F\\u0633\\u062A\\u0645 \\u0631\\u0633\\u06CC\\u062F\\u0647 \\u0639\\u0627\\u0644\\u06CC\\u0647 \\u0645\\u0631\\u0633\\u06CC \\u0627\\u0632 \\u062F\\u06CC\\u062C\\u06CC \\u06A9\\u0627\\u0644\\u0627\" #@param {type:\"string\"}\n",
        "from IPython.core.display import display, HTML\n",
        "if not user_text==\"\":\n",
        "  text_for_test = _normalizer.normalize(user_text)\n",
        "  text_for_test_words = _wordtokenizer.tokenize(text_for_test)\n",
        "  x_text_for_test_words = np.zeros((1,max_no_tokens,vector_size),dtype=K.floatx())\n",
        "  for t in range(0,len(text_for_test_words)):\n",
        "    if t >= max_no_tokens:\n",
        "      break\n",
        "    if text_for_test_words[t] not in w2v_model.words:\n",
        "      continue\n",
        "    \n",
        "    x_text_for_test_words[0, t, :] = w2v_model.get_word_vector(text_for_test_words[t])\n",
        "  # print(x_text_for_test_words.shape)\n",
        "  # print(text_for_test_words)\n",
        "  result = model.predict(x_text_for_test_words)\n",
        "  pos_percent = str(int(result[0][1]*100))+\" % \"\n",
        "  neg_percent = str(int(result[0][0]*100))+\" % \"\n",
        "  display(HTML(\"<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>{}</h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>{}</h4></div></div>\".format(pos_percent,neg_percent)))\n",
        "else:\n",
        "  print(\"Please enter your text\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style='text-align: center'><div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/positive.png'/><h4>93 % </h4></div> | <div style='display:inline-block'><img src='https://www.paralleldots.com/static/images/negative.png'/><h4>6 % </h4></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}